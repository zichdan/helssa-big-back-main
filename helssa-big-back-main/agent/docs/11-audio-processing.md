# ğŸ™ï¸ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª Ùˆ STT Ø¯Ø± HELSSA

## ğŸ“‹ ÙÙ‡Ø±Ø³Øª Ù…Ø·Ø§Ù„Ø¨

- [Ù…Ø¹Ø±ÙÛŒ Ø³ÛŒØ³ØªÙ… Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª](## ğŸ¯ Ù…Ø¹Ø±ÙÛŒ Ø³ÛŒØ³ØªÙ… Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª)
- [Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª](## ğŸ—ï¸ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª)
- [Ù…Ø¯ÛŒØ±ÛŒØª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØµÙˆØªÛŒ](## ğŸ“ Ù…Ø¯ÛŒØ±ÛŒØª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØµÙˆØªÛŒ)
- [Ø³Ø±ÙˆÛŒØ³ Whisper STT](## ğŸ“ Ø³Ø±ÙˆÛŒØ³ Whisper STT)
- [Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‚Ø·Ø¹Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ØµÙˆØª](## ğŸ“ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‚Ø·Ø¹Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ØµÙˆØª)
- [Ø§Ø¯ØºØ§Ù… Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒâ€ŒÙ‡Ø§](## ğŸ“ Ø§Ø¯ØºØ§Ù… Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒâ€ŒÙ‡Ø§)
- [Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ú©ÛŒÙÛŒØª](## ğŸ“ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ú©ÛŒÙÛŒØª)
- [Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ùˆ Ú¯Ø²Ø§Ø±Ø´â€ŒØ¯Ù‡ÛŒ](## ğŸ“ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ùˆ Ú¯Ø²Ø§Ø±Ø´â€ŒØ¯Ù‡ÛŒ)

---

## ğŸ¯ Ù…Ø¹Ø±ÙÛŒ Ø³ÛŒØ³ØªÙ… Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª

Ø³ÛŒØ³ØªÙ… Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª HELSSA ÛŒÚ© Ù¾Ù„ØªÙØ±Ù… Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ø¶Ø¨Ø·ØŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ Ù…Ú©Ø§Ù„Ù…Ø§Øª Ù¾Ø²Ø´Ú©ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø§Ø² Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ AI Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

### ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ

- ğŸ™ï¸ **Ø¶Ø¨Ø· Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§** Ù…Ú©Ø§Ù„Ù…Ø§Øª Ù¾Ø²Ø´Ú©ÛŒ
- ğŸ”„ **Ù¾Ø±Ø¯Ø§Ø²Ø´ Real-time** ØµÙˆØª Ø¯Ø± Ø­ÛŒÙ† Ø¶Ø¨Ø·
- ğŸ“ **Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ Ø¯Ù‚ÛŒÙ‚** Ø¨Ø§ Whisper OpenAI
- ğŸ” **Ø±Ù…Ø²Ù†Ú¯Ø§Ø±ÛŒ End-to-End** ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØµÙˆØªÛŒ
- ğŸ“Š **ØªØ´Ø®ÛŒØµ Ú¯ÙˆÛŒÙ†Ø¯Ù‡** (Speaker Diarization)
- ğŸŒ **Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ú†Ù†Ø¯Ø²Ø¨Ø§Ù†Ù‡** (ÙØ§Ø±Ø³ÛŒØŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒØŒ Ø¹Ø±Ø¨ÛŒ)
- âš¡ **Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ** Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§
- ğŸ“ˆ **Ø¯Ù‚Øª 98%+** Ø¯Ø± Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ ÙØ§Ø±Ø³ÛŒ

## ğŸ—ï¸ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª

```mermaid
graph TB
    subgraph "Audio Input Layer"
        MIC[Microphone Input]
        UPLOAD[File Upload]
        STREAM[WebRTC Stream]
    end
    
    subgraph "Processing Pipeline"
        RECV[Audio Receiver]
        VALID[Validator]
        CHUNK[Chunker]
        QUEUE[Task Queue]
    end
    
    subgraph "STT Processing"
        WORKER1[STT Worker 1]
        WORKER2[STT Worker 2]
        WORKER3[STT Worker N]
        WHISPER[Whisper API]
    end
    
    subgraph "Post Processing"
        MERGE[Transcript Merger]
        DIAR[Speaker Diarization]
        CORRECT[Error Correction]
        FORMAT[Formatter]
    end
    
    subgraph "Storage Layer"
        MINIO[(MinIO Storage)]
        REDIS[(Redis Cache)]
        DB[(Database)]
    end
    
    MIC --> RECV
    UPLOAD --> RECV
    STREAM --> RECV
    
    RECV --> VALID
    VALID --> CHUNK
    CHUNK --> QUEUE
    
    QUEUE --> WORKER1
    QUEUE --> WORKER2
    QUEUE --> WORKER3
    
    WORKER1 --> WHISPER
    WORKER2 --> WHISPER
    WORKER3 --> WHISPER
    
    WHISPER --> MERGE
    MERGE --> DIAR
    DIAR --> CORRECT
    CORRECT --> FORMAT
    
    CHUNK --> MINIO
    WHISPER --> REDIS
    FORMAT --> DB
```

### Ù†Ù…ÙˆØ¯Ø§Ø± Ø¬Ø±ÛŒØ§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´

```mermaid
sequenceDiagram
    participant Client
    participant API
    participant Chunker
    participant Queue
    participant STTWorker
    participant Whisper
    participant Storage
    
    Client->>API: Upload Audio
    API->>API: Validate & Encrypt
    API->>Chunker: Process Audio
    Chunker->>Storage: Store Chunks
    Chunker->>Queue: Queue STT Tasks
    
    loop For Each Chunk
        Queue->>STTWorker: Process Chunk
        STTWorker->>Whisper: Transcribe
        Whisper-->>STTWorker: Transcript
        STTWorker->>Storage: Save Result
    end
    
    STTWorker->>API: Merge Results
    API-->>Client: Complete Transcript
```

### Ø³Ø§Ø®ØªØ§Ø± Ù¾Ø±ÙˆÚ˜Ù‡ STT

```python
stt/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ stt_job.py              # Ù…Ø¯Ù„ Ú©Ø§Ø± STT
â”‚   â”œâ”€â”€ audio_file.py           # Ù…Ø¯Ù„ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
â”‚   â”œâ”€â”€ transcript_segment.py   # Ù…Ø¯Ù„ Ù‚Ø·Ø¹Ø§Øª Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ
â”‚   â””â”€â”€ stt_config.py           # ØªÙ†Ø¸ÛŒÙ…Ø§Øª STT
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ audio_receiver.py       # Ø¯Ø±ÛŒØ§ÙØªâ€ŒÚ©Ù†Ù†Ø¯Ù‡ ØµÙˆØª
â”‚   â”œâ”€â”€ audio_validator.py      # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
â”‚   â”œâ”€â”€ audio_chunker.py        # Ù‚Ø·Ø¹Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
â”‚   â”œâ”€â”€ whisper_service.py      # Ø³Ø±ÙˆÛŒØ³ Whisper
â”‚   â”œâ”€â”€ transcript_merger.py    # Ø§Ø¯ØºØ§Ù… Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ
â”‚   â””â”€â”€ speaker_diarization.py  # ØªØ´Ø®ÛŒØµ Ú¯ÙˆÛŒÙ†Ø¯Ù‡
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ audio_preprocessor.py   # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
â”‚   â”œâ”€â”€ noise_reduction.py      # Ú©Ø§Ù‡Ø´ Ù†ÙˆÛŒØ²
â”‚   â”œâ”€â”€ format_converter.py     # ØªØ¨Ø¯ÛŒÙ„ ÙØ±Ù…Øª
â”‚   â””â”€â”€ quality_enhancer.py     # Ø¨Ù‡Ø¨ÙˆØ¯ Ú©ÛŒÙÛŒØª
â”œâ”€â”€ tasks.py                     # Celery tasks
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ audio_utils.py          # Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ ØµÙˆØªÛŒ
â”‚   â”œâ”€â”€ encryption.py           # Ø±Ù…Ø²Ù†Ú¯Ø§Ø±ÛŒ
â”‚   â””â”€â”€ metrics.py              # Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
â””â”€â”€ api/
    â”œâ”€â”€ views.py
    â””â”€â”€ serializers.py
```

## ğŸ“ Ù…Ø¯ÛŒØ±ÛŒØª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØµÙˆØªÛŒ

### Audio File Model

```python
# stt/models/audio_file.py
from django.db import models
import uuid
from datetime import datetime

class AudioFile(models.Model):
    """Ù…Ø¯Ù„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØµÙˆØªÛŒ"""
    
    FILE_STATUS = [
        ('uploading', 'Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù¾Ù„ÙˆØ¯'),
        ('uploaded', 'Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡'),
        ('processing', 'Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´'),
        ('completed', 'ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯Ù‡'),
        ('failed', 'Ù†Ø§Ù…ÙˆÙÙ‚'),
    ]
    
    AUDIO_FORMATS = [
        ('webm', 'WebM'),
        ('mp3', 'MP3'),
        ('wav', 'WAV'),
        ('m4a', 'M4A'),
        ('ogg', 'OGG'),
        ('flac', 'FLAC'),
    ]
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4)
    encounter = models.ForeignKey(
        'encounters.Encounter',
        on_delete=models.CASCADE,
        related_name='audio_files'
    )
    
    # Ù…Ø´Ø®ØµØ§Øª ÙØ§ÛŒÙ„
    original_name = models.CharField(max_length=255)
    file_format = models.CharField(
        max_length=10,
        choices=AUDIO_FORMATS
    )
    file_size = models.BigIntegerField(help_text="Ø­Ø¬Ù… Ø¨Ù‡ Ø¨Ø§ÛŒØª")
    duration_seconds = models.FloatField(
        null=True,
        blank=True,
        help_text="Ù…Ø¯Øª Ø²Ù…Ø§Ù† Ø¨Ù‡ Ø«Ø§Ù†ÛŒÙ‡"
    )
    
    # Ú©ÛŒÙÛŒØª ØµÙˆØª
    sample_rate = models.IntegerField(
        default=48000,
        help_text="Ù†Ø±Ø® Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ"
    )
    bit_rate = models.IntegerField(
        null=True,
        blank=True,
        help_text="Ù†Ø±Ø® Ø¨ÛŒØª"
    )
    channels = models.IntegerField(
        default=1,
        help_text="ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§"
    )
    
    # Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ
    storage_path = models.CharField(max_length=500)
    storage_url = models.URLField()
    is_encrypted = models.BooleanField(default=True)
    encryption_key_id = models.CharField(
        max_length=100,
        help_text="Ø´Ù†Ø§Ø³Ù‡ Ú©Ù„ÛŒØ¯ Ø±Ù…Ø²Ù†Ú¯Ø§Ø±ÛŒ"
    )
    
    # ÙˆØ¶Ø¹ÛŒØª
    status = models.CharField(
        max_length=20,
        choices=FILE_STATUS,
        default='uploading'
    )
    upload_progress = models.IntegerField(
        default=0,
        help_text="Ø¯Ø±ØµØ¯ Ø¢Ù¾Ù„ÙˆØ¯"
    )
    
    # Ù…ØªØ§Ø¯Ø§Ø¯Ù‡
    metadata = models.JSONField(
        default=dict,
        help_text="Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÛŒ ÙØ§ÛŒÙ„"
    )
    
    # Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§
    created_at = models.DateTimeField(auto_now_add=True)
    uploaded_at = models.DateTimeField(null=True, blank=True)
    processing_started_at = models.DateTimeField(null=True, blank=True)
    processing_completed_at = models.DateTimeField(null=True, blank=True)
    
    class Meta:
        db_table = 'audio_files'
        indexes = [
            models.Index(fields=['encounter', 'status']),
            models.Index(fields=['created_at']),
        ]
```

### Audio Receiver Service

```python
# stt/services/audio_receiver.py
from typing import Dict, Optional
import asyncio
import hashlib
from django.core.files.uploadedfile import UploadedFile

class AudioReceiverService:
    """Ø³Ø±ÙˆÛŒØ³ Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØµÙˆØªÛŒ"""
    
    def __init__(self):
        self.validator = AudioValidator()
        self.storage = MinIOStorageService()
        self.encryptor = AudioEncryptionService()
        self.metadata_extractor = AudioMetadataExtractor()
        
    async def receive_audio(
        self,
        encounter_id: str,
        audio_file: UploadedFile,
        user_id: str
    ) -> AudioFile:
        """Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§ÙˆÙ„ÛŒÙ‡ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ"""
        
        # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        validation_result = await self.validator.validate(audio_file)
        if not validation_result['valid']:
            raise AudioValidationError(validation_result['errors'])
            
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ§Ø¯Ø§Ø¯Ù‡
        metadata = await self.metadata_extractor.extract(audio_file)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯
        audio_record = await AudioFile.objects.create(
            encounter_id=encounter_id,
            original_name=audio_file.name,
            file_format=metadata['format'],
            file_size=audio_file.size,
            duration_seconds=metadata.get('duration'),
            sample_rate=metadata.get('sample_rate', 48000),
            bit_rate=metadata.get('bit_rate'),
            channels=metadata.get('channels', 1),
            status='uploading',
            metadata=metadata
        )
        
        # Ø¢Ù¾Ù„ÙˆØ¯ Ø¨Ø§ streaming
        await self._stream_upload(audio_record, audio_file)
        
        return audio_record
        
    async def _stream_upload(
        self,
        audio_record: AudioFile,
        audio_file: UploadedFile
    ):
        """Ø¢Ù¾Ù„ÙˆØ¯ Ø§Ø³ØªØ±ÛŒÙ…ÛŒÙ†Ú¯ Ø¨Ø§ Ø±Ù…Ø²Ù†Ú¯Ø§Ø±ÛŒ"""
        
        # ØªÙˆÙ„ÛŒØ¯ Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡
        storage_path = f"audio/{audio_record.encounter_id}/{audio_record.id}.enc"
        
        # Ø±Ù…Ø²Ù†Ú¯Ø§Ø±ÛŒ Ùˆ Ø¢Ù¾Ù„ÙˆØ¯
        chunk_size = 1024 * 1024  # 1MB chunks
        total_size = audio_file.size
        uploaded = 0
        
        # Ø§ÛŒØ¬Ø§Ø¯ multipart upload
        upload_id = await self.storage.initiate_multipart_upload(
            storage_path
        )
        
        parts = []
        part_number = 1
        
        # Ø®ÙˆØ§Ù†Ø¯Ù† Ùˆ Ø±Ù…Ø²Ù†Ú¯Ø§Ø±ÛŒ chunk by chunk
        encryptor = self.encryptor.create_stream_encryptor(
            audio_record.id
        )
        
        while True:
            chunk = audio_file.read(chunk_size)
            if not chunk:
                break
                
            # Ø±Ù…Ø²Ù†Ú¯Ø§Ø±ÛŒ chunk
            encrypted_chunk = await encryptor.encrypt_chunk(chunk)
            
            # Ø¢Ù¾Ù„ÙˆØ¯ part
            etag = await self.storage.upload_part(
                storage_path,
                upload_id,
                part_number,
                encrypted_chunk
            )
            
            parts.append({
                'PartNumber': part_number,
                'ETag': etag
            })
            
            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾ÛŒØ´Ø±ÙØª
            uploaded += len(chunk)
            progress = int((uploaded / total_size) * 100)
            audio_record.upload_progress = progress
            await audio_record.save()
            
            part_number += 1
            
        # ØªÚ©Ù…ÛŒÙ„ multipart upload
        await self.storage.complete_multipart_upload(
            storage_path,
            upload_id,
            parts
        )
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø±Ú©ÙˆØ±Ø¯
        audio_record.storage_path = storage_path
        audio_record.storage_url = self.storage.get_url(storage_path)
        audio_record.encryption_key_id = encryptor.key_id
        audio_record.status = 'uploaded'
        audio_record.uploaded_at = timezone.now()
        await audio_record.save()
        
        # Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´
        await process_audio_file.delay(str(audio_record.id))
```

### Audio Validator

```python
# stt/services/audio_validator.py
import magic
from pydub import AudioSegment

class AudioValidator:
    """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØµÙˆØªÛŒ"""
    
    def __init__(self):
        self.max_file_size = 500 * 1024 * 1024  # 500MB
        self.max_duration = 3600  # 1 hour
        self.allowed_formats = [
            'audio/webm',
            'audio/mpeg',
            'audio/mp3',
            'audio/wav',
            'audio/x-wav',
            'audio/mp4',
            'audio/ogg',
            'audio/flac'
        ]
        self.min_sample_rate = 16000
        self.max_sample_rate = 48000
        
    async def validate(self, audio_file) -> Dict:
        """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¬Ø§Ù…Ø¹ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ"""
        
        errors = []
        warnings = []
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø­Ø¬Ù…
        if audio_file.size > self.max_file_size:
            errors.append(
                f"Ø­Ø¬Ù… ÙØ§ÛŒÙ„ ({audio_file.size / 1024 / 1024:.1f}MB) "
                f"Ø§Ø² Ø­Ø¯ Ù…Ø¬Ø§Ø² ({self.max_file_size / 1024 / 1024}MB) Ø¨ÛŒØ´ØªØ± Ø§Ø³Øª"
            )
            
        # Ø¨Ø±Ø±Ø³ÛŒ Ù†ÙˆØ¹ ÙØ§ÛŒÙ„
        mime_type = magic.from_buffer(
            audio_file.read(1024),
            mime=True
        )
        audio_file.seek(0)
        
        if mime_type not in self.allowed_formats:
            errors.append(
                f"ÙØ±Ù…Øª ÙØ§ÛŒÙ„ ({mime_type}) Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯"
            )
            
        # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ ØµÙˆØªÛŒ
        try:
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ø§ pydub
            audio = AudioSegment.from_file(audio_file)
            audio_file.seek(0)
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Øª Ø²Ù…Ø§Ù†
            duration = len(audio) / 1000  # Ø¨Ù‡ Ø«Ø§Ù†ÛŒÙ‡
            if duration > self.max_duration:
                errors.append(
                    f"Ù…Ø¯Øª Ø²Ù…Ø§Ù† ÙØ§ÛŒÙ„ ({duration:.1f} Ø«Ø§Ù†ÛŒÙ‡) "
                    f"Ø§Ø² Ø­Ø¯ Ù…Ø¬Ø§Ø² ({self.max_duration} Ø«Ø§Ù†ÛŒÙ‡) Ø¨ÛŒØ´ØªØ± Ø§Ø³Øª"
                )
                
            # Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø±Ø® Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ
            if audio.frame_rate < self.min_sample_rate:
                warnings.append(
                    f"Ù†Ø±Ø® Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ ({audio.frame_rate}Hz) "
                    f"Ú©Ù…ØªØ± Ø§Ø² Ø­Ø¯ ØªÙˆØµÛŒÙ‡ Ø´Ø¯Ù‡ ({self.min_sample_rate}Hz) Ø§Ø³Øª. "
                    "Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú©ÛŒÙÛŒØª Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ Ú©Ø§Ù‡Ø´ ÛŒØ§Ø¨Ø¯."
                )
                
            # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø·Ø­ ØµØ¯Ø§
            if audio.dBFS < -40:
                warnings.append(
                    "Ø³Ø·Ø­ ØµØ¯Ø§ÛŒ ÙØ§ÛŒÙ„ Ø¨Ø³ÛŒØ§Ø± Ù¾Ø§ÛŒÛŒÙ† Ø§Ø³Øª. "
                    "Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ù‚Øª Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ Ú©Ø§Ù‡Ø´ ÛŒØ§Ø¨Ø¯."
                )
                
        except Exception as e:
            errors.append(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ: {str(e)}")
            
        return {
            'valid': len(errors) == 0,
            'errors': errors,
            'warnings': warnings
        }
```

## ğŸ¤ Ø³Ø±ÙˆÛŒØ³ Whisper STT

### Whisper Service Implementation

```python
# stt/services/whisper_service.py
import asyncio
from typing import Dict, List, Optional
import openai
from tenacity import retry, stop_after_attempt, wait_exponential

class WhisperSTTService:
    """Ø³Ø±ÙˆÛŒØ³ ØªØ¨Ø¯ÛŒÙ„ Ú¯ÙØªØ§Ø± Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Whisper"""
    
    def __init__(self):
        self.client = openai.AsyncOpenAI(
            api_key=settings.OPENAI_API_KEY,
            base_url=settings.OPENAI_BASE_URL  # For GapGPT compatibility
        )
        self.model = "whisper-1"
        self.supported_languages = ['fa', 'en', 'ar']
        self.medical_prompt = self._load_medical_prompt()
        
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def transcribe(
        self,
        audio_data: bytes,
        language: str = 'fa',
        prompt: Optional[str] = None,
        temperature: float = 0.0
    ) -> Dict:
        """Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ"""
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø±Ø§Ù…ØªØ±Ù‡Ø§
        if language not in self.supported_languages:
            language = 'fa'
            
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² prompt Ù¾Ø²Ø´Ú©ÛŒ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… Ø§Ø±Ø§Ø¦Ù‡
        if not prompt:
            prompt = self.medical_prompt.get(language, '')
            
        try:
            # Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Whisper API
            response = await self.client.audio.transcriptions.create(
                model=self.model,
                file=("audio.mp3", audio_data, "audio/mpeg"),
                language=language,
                prompt=prompt,
                temperature=temperature,
                response_format="verbose_json"
            )
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø§Ø³Ø®
            return self._process_response(response)
            
        except openai.APIError as e:
            # Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§Ù‡Ø§ÛŒ API
            if e.status_code == 429:  # Rate limit
                raise STTRateLimitError("Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ù†Ø±Ø® Ø¯Ø±Ø®ÙˆØ§Ø³Øª")
            elif e.status_code == 413:  # File too large
                raise STTFileTooLargeError("Ø­Ø¬Ù… ÙØ§ÛŒÙ„ Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ Ù…Ø¬Ø§Ø²")
            else:
                raise STTProcessingError(f"Ø®Ø·Ø§ÛŒ API: {str(e)}")
                
    def _process_response(self, response) -> Dict:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø§Ø³Ø® Whisper"""
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ confidence score
        confidence = self._calculate_confidence(response)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ segments Ø¨Ø§ timestamps
        segments = []
        if hasattr(response, 'segments'):
            for segment in response.segments:
                segments.append({
                    'id': segment.id,
                    'start': segment.start,
                    'end': segment.end,
                    'text': segment.text,
                    'confidence': segment.get('confidence', confidence),
                    'words': self._extract_words(segment)
                })
                
        return {
            'text': response.text,
            'language': response.language,
            'duration': response.duration,
            'segments': segments,
            'confidence': confidence,
            'metadata': {
                'model': self.model,
                'temperature': response.temperature
            }
        }
        
    def _calculate_confidence(self, response) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø§Ø·Ù…ÛŒÙ†Ø§Ù†"""
        
        # Whisper Ø§Ù…ØªÛŒØ§Ø² Ù…Ø³ØªÙ‚ÛŒÙ… Ù†Ø¯Ø§Ø±Ø¯ØŒ ØªØ®Ù…ÛŒÙ† Ù…ÛŒâ€ŒØ²Ù†ÛŒÙ…
        confidence = 0.85  # Ù¾Ø§ÛŒÙ‡
        
        # Ø§ÙØ²Ø§ÛŒØ´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·ÙˆÙ„ Ù…ØªÙ†
        if len(response.text) > 100:
            confidence += 0.05
            
        # Ú©Ø§Ù‡Ø´ Ø§Ú¯Ø± Ø²Ø¨Ø§Ù† ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ù…ØªÙØ§ÙˆØª Ø§Ø³Øª
        if hasattr(response, 'language_probability'):
            confidence *= response.language_probability
            
        return min(confidence, 0.99)
        
    def _load_medical_prompt(self) -> Dict[str, str]:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ prompt Ù‡Ø§ÛŒ Ù¾Ø²Ø´Ú©ÛŒ"""
        
        return {
            'fa': """Ø§ÛŒÙ† ÛŒÚ© Ù…Ú©Ø§Ù„Ù…Ù‡ Ù¾Ø²Ø´Ú©ÛŒ Ø¨ÛŒÙ† Ù¾Ø²Ø´Ú© Ùˆ Ø¨ÛŒÙ…Ø§Ø± Ø§Ø³Øª.
                    Ø§ØµØ·Ù„Ø§Ø­Ø§Øª Ù¾Ø²Ø´Ú©ÛŒ Ø±Ø§ÛŒØ¬: Ù‚Ø±ØµØŒ Ø¢Ù…Ù¾ÙˆÙ„ØŒ Ø¢Ø²Ù…Ø§ÛŒØ´ØŒ Ù…Ø¹Ø§ÛŒÙ†Ù‡ØŒ ØªØ´Ø®ÛŒØµØŒ Ø¯Ø±Ù…Ø§Ù†ØŒ Ù†Ø³Ø®Ù‡ØŒ Ø¯Ø§Ø±Ùˆ
                    Ø¯Ø§Ø±ÙˆÙ‡Ø§ÛŒ Ø±Ø§ÛŒØ¬: Ø§Ø³ØªØ§Ù…ÛŒÙ†ÙˆÙÙ†ØŒ Ø¢Ù…ÙˆÚ©Ø³ÛŒâ€ŒØ³ÛŒÙ„ÛŒÙ†ØŒ Ù…ØªÙÙˆØ±Ù…ÛŒÙ†ØŒ Ù„ÙˆØ²Ø§Ø±ØªØ§Ù†ØŒ Ø¢Ø³Ù¾Ø±ÛŒÙ†
                    Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒØ¬: Ø¯ÛŒØ§Ø¨ØªØŒ ÙØ´Ø§Ø± Ø®ÙˆÙ†ØŒ Ø³Ø±Ù…Ø§Ø®ÙˆØ±Ø¯Ú¯ÛŒØŒ Ø¢Ù†ÙÙˆÙ„Ø§Ù†Ø²Ø§ØŒ Ù…ÛŒÚ¯Ø±Ù†""",
                    
            'en': """This is a medical conversation between a doctor and patient.
                    Common medical terms: prescription, diagnosis, treatment, examination, medication
                    Common drugs: acetaminophen, amoxicillin, metformin, losartan, aspirin
                    Common conditions: diabetes, hypertension, cold, flu, migraine""",
                    
            'ar': """Ù‡Ø°Ù‡ Ù…Ø­Ø§Ø¯Ø«Ø© Ø·Ø¨ÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ø·Ø¨ÙŠØ¨ ÙˆØ§Ù„Ù…Ø±ÙŠØ¶.
                    Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©: ÙˆØµÙØ© Ø·Ø¨ÙŠØ©ØŒ ØªØ´Ø®ÙŠØµØŒ Ø¹Ù„Ø§Ø¬ØŒ ÙØ­ØµØŒ Ø¯ÙˆØ§Ø¡
                    Ø§Ù„Ø£Ø¯ÙˆÙŠØ© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©: Ø£Ø³ÙŠØªØ§Ù…ÙŠÙ†ÙˆÙÙŠÙ†ØŒ Ø£Ù…ÙˆÙƒØ³ÙŠØ³ÙŠÙ„ÙŠÙ†ØŒ Ù…ÙŠØªÙÙˆØ±Ù…ÙŠÙ†ØŒ Ù„ÙˆØ²Ø§Ø±ØªØ§Ù†ØŒ Ø£Ø³Ø¨Ø±ÙŠÙ†"""
        }
```

### STT Job Manager

```python
# stt/services/stt_job_manager.py

class STTJobManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ø§Ø±Ù‡Ø§ÛŒ STT"""
    
    def __init__(self):
        self.whisper_service = WhisperSTTService()
        self.queue_manager = QueueManager()
        self.progress_tracker = ProgressTracker()
        
    async def create_stt_job(
        self,
        audio_file_id: str,
        priority: str = 'normal'
    ) -> STTJob:
        """Ø§ÛŒØ¬Ø§Ø¯ Ú©Ø§Ø± STT Ø¬Ø¯ÛŒØ¯"""
        
        audio_file = await AudioFile.objects.get(id=audio_file_id)
        
        # Ø§ÛŒØ¬Ø§Ø¯ job
        job = await STTJob.objects.create(
            audio_file=audio_file,
            status='pending',
            priority=priority,
            total_chunks=0,  # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
            metadata={
                'encounter_id': str(audio_file.encounter_id),
                'language': 'fa',  # ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± ÛŒØ§ Ø§Ø² encounter
            }
        )
        
        # Ù‚Ø·Ø¹Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙØ§ÛŒÙ„
        chunks = await self._create_audio_chunks(audio_file)
        job.total_chunks = len(chunks)
        await job.save()
        
        # Ø§ÛŒØ¬Ø§Ø¯ task Ù‡Ø§ÛŒ STT
        for i, chunk in enumerate(chunks):
            await self.queue_manager.enqueue_stt_task(
                job_id=str(job.id),
                chunk_id=str(chunk.id),
                priority=priority
            )
            
        return job
        
    async def _create_audio_chunks(
        self,
        audio_file: AudioFile
    ) -> List[AudioChunk]:
        """Ù‚Ø·Ø¹Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ"""
        
        chunker = AudioChunker()
        
        # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø±Ù…Ø²Ú¯Ø´Ø§ÛŒÛŒ ÙØ§ÛŒÙ„
        encrypted_data = await download_from_storage(
            audio_file.storage_path
        )
        
        decrypted_data = await decrypt_audio(
            encrypted_data,
            audio_file.encryption_key_id
        )
        
        # Ù‚Ø·Ø¹Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        chunk_configs = await chunker.split_audio(
            decrypted_data,
            audio_file.file_format,
            chunk_duration=60,  # 60 Ø«Ø§Ù†ÛŒÙ‡
            overlap=2  # 2 Ø«Ø§Ù†ÛŒÙ‡ Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ
        )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù‚Ø·Ø¹Ø§Øª
        chunks = []
        for i, config in enumerate(chunk_configs):
            chunk = await AudioChunk.objects.create(
                encounter_id=audio_file.encounter_id,
                audio_file=audio_file,
                chunk_index=i,
                start_time=config['start'],
                end_time=config['end'],
                duration=config['duration'],
                file_url=config['url'],
                status='pending'
            )
            chunks.append(chunk)
            
        return chunks
```

## ğŸ”„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‚Ø·Ø¹Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ØµÙˆØª

### Audio Chunker Service

```python
# stt/services/audio_chunker.py
from pydub import AudioSegment
import io
from typing import List, Dict

class AudioChunker:
    """Ø³Ø±ÙˆÛŒØ³ Ù‚Ø·Ø¹Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØµÙˆØªÛŒ"""
    
    def __init__(self):
        self.storage = MinIOStorageService()
        self.default_chunk_duration = 60  # seconds
        self.default_overlap = 2  # seconds
        
    async def split_audio(
        self,
        audio_data: bytes,
        format: str,
        chunk_duration: int = None,
        overlap: int = None
    ) -> List[Dict]:
        """ØªÙ‚Ø³ÛŒÙ… ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø¨Ù‡ Ù‚Ø·Ø¹Ø§Øª"""
        
        chunk_duration = chunk_duration or self.default_chunk_duration
        overlap = overlap or self.default_overlap
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙˆØª
        audio = AudioSegment.from_file(
            io.BytesIO(audio_data),
            format=format
        )
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
        chunk_length_ms = chunk_duration * 1000
        overlap_ms = overlap * 1000
        total_duration_ms = len(audio)
        
        chunks = []
        start_ms = 0
        
        while start_ms < total_duration_ms:
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†ØªÙ‡Ø§
            end_ms = min(start_ms + chunk_length_ms, total_duration_ms)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚Ø·Ø¹Ù‡
            chunk_audio = audio[start_ms:end_ms]
            
            # Ø§ÙØ²ÙˆØ¯Ù† fade in/out Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ artifacts
            if start_ms > 0:
                chunk_audio = chunk_audio.fade_in(100)
            if end_ms < total_duration_ms:
                chunk_audio = chunk_audio.fade_out(100)
                
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ mp3 Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ù‡ØªØ±
            chunk_buffer = io.BytesIO()
            chunk_audio.export(
                chunk_buffer,
                format='mp3',
                parameters=['-q:a', '0']  # Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ú©ÛŒÙÛŒØª
            )
            
            # Ø¢Ù¾Ù„ÙˆØ¯ Ù‚Ø·Ø¹Ù‡
            chunk_url = await self._upload_chunk(
                chunk_buffer.getvalue(),
                f"chunk_{start_ms}_{end_ms}.mp3"
            )
            
            chunks.append({
                'start': start_ms / 1000,
                'end': end_ms / 1000,
                'duration': (end_ms - start_ms) / 1000,
                'url': chunk_url,
                'overlap_start': max(0, (start_ms - overlap_ms) / 1000),
                'overlap_end': min(total_duration_ms, (end_ms + overlap_ms) / 1000)
            })
            
            # Ø­Ø±Ú©Øª Ø¨Ù‡ Ù‚Ø·Ø¹Ù‡ Ø¨Ø¹Ø¯ÛŒ Ø¨Ø§ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† overlap
            start_ms = end_ms - overlap_ms
            
        return chunks
        
    async def _upload_chunk(
        self,
        chunk_data: bytes,
        filename: str
    ) -> str:
        """Ø¢Ù¾Ù„ÙˆØ¯ Ù‚Ø·Ø¹Ù‡ Ø¨Ù‡ storage"""
        
        path = f"temp/chunks/{filename}"
        await self.storage.upload(path, chunk_data)
        return self.storage.get_url(path)
```

### Chunk Processing Task

```python
# stt/tasks.py
from celery import shared_task
import asyncio

@shared_task(
    bind=True,
    queue='stt',
    max_retries=3,
    default_retry_delay=60
)
def process_audio_chunk(self, job_id: str, chunk_id: str):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ù‚Ø·Ø¹Ù‡ ØµÙˆØªÛŒ"""
    
    try:
        # Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
        chunk = AudioChunk.objects.get(id=chunk_id)
        job = STTJob.objects.get(id=job_id)
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆØ¶Ø¹ÛŒØª
        chunk.status = 'processing'
        chunk.processing_started_at = timezone.now()
        chunk.save()
        
        # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‚Ø·Ø¹Ù‡
        audio_data = download_from_url(chunk.file_url)
        
        # Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ Ø¨Ø§ Whisper
        whisper_service = WhisperSTTService()
        result = asyncio.run(
            whisper_service.transcribe(
                audio_data,
                language=job.metadata.get('language', 'fa'),
                temperature=0.0  # Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ±
            )
        )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªÛŒØ¬Ù‡
        TranscriptSegment.objects.create(
            audio_chunk=chunk,
            text=result['text'],
            language=result['language'],
            confidence=result['confidence'],
            segments=result['segments'],
            words=result.get('words', []),
            processing_time=result.get('duration')
        )
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ chunk
        chunk.status = 'completed'
        chunk.processing_completed_at = timezone.now()
        chunk.save()
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ job progress
        update_job_progress.delay(job_id)
        
        return {
            'chunk_id': chunk_id,
            'status': 'success',
            'text_length': len(result['text'])
        }
        
    except Exception as e:
        # Ø«Ø¨Øª Ø®Ø·Ø§
        chunk.status = 'failed'
        chunk.error_message = str(e)
        chunk.save()
        
        # retry logic
        if self.request.retries < self.max_retries:
            raise self.retry(exc=e)
        else:
            # Ù†Ù‡Ø§ÛŒØªØ§Ù‹ failed
            mark_job_failed.delay(job_id, str(e))
            raise
```

## ğŸ”€ Ø§Ø¯ØºØ§Ù… Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒâ€ŒÙ‡Ø§

### Transcript Merger Service

```python
# stt/services/transcript_merger.py
from typing import List, Dict
import difflib

class TranscriptMerger:
    """Ø³Ø±ÙˆÛŒØ³ Ø§Ø¯ØºØ§Ù… Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‚Ø·Ø¹Ø§Øª"""
    
    def __init__(self):
        self.overlap_handler = OverlapHandler()
        self.text_aligner = TextAligner()
        self.quality_checker = QualityChecker()
        
    async def merge_transcripts(
        self,
        job_id: str
    ) -> str:
        """Ø§Ø¯ØºØ§Ù… ØªÙ…Ø§Ù… Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒâ€ŒÙ‡Ø§ÛŒ ÛŒÚ© job"""
        
        # Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ segments
        segments = await TranscriptSegment.objects.filter(
            audio_chunk__audio_file__stt_job__id=job_id
        ).order_by('audio_chunk__chunk_index').select_related(
            'audio_chunk'
        )
        
        if not segments:
            raise NoTranscriptsError("Ù‡ÛŒÚ† Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
            
        # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ overlap
        grouped_segments = self._group_overlapping_segments(segments)
        
        # Ø§Ø¯ØºØ§Ù… Ù‡Ø± Ú¯Ø±ÙˆÙ‡
        merged_texts = []
        for group in grouped_segments:
            if len(group) == 1:
                # Ø¨Ø¯ÙˆÙ† overlap
                merged_texts.append(group[0].text)
            else:
                # Ø§Ø¯ØºØ§Ù… Ø¨Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ØªØ·Ø¨ÛŒÙ‚
                merged = await self._merge_overlapping_texts(group)
                merged_texts.append(merged)
                
        # ØªØ±Ú©ÛŒØ¨ Ù†Ù‡Ø§ÛŒÛŒ
        final_text = ' '.join(merged_texts)
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ú©ÛŒÙÛŒØª
        quality_score = await self.quality_checker.check(
            final_text,
            segments
        )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªÛŒØ¬Ù‡
        await self._save_merged_transcript(
            job_id,
            final_text,
            quality_score
        )
        
        return final_text
        
    def _group_overlapping_segments(
        self,
        segments: List[TranscriptSegment]
    ) -> List[List[TranscriptSegment]]:
        """Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ segments Ø¨Ø§ overlap"""
        
        groups = []
        current_group = []
        
        for i, segment in enumerate(segments):
            if i == 0:
                current_group.append(segment)
                continue
                
            # Ø¨Ø±Ø±Ø³ÛŒ overlap Ø¨Ø§ segment Ù‚Ø¨Ù„ÛŒ
            prev_chunk = segments[i-1].audio_chunk
            curr_chunk = segment.audio_chunk
            
            if curr_chunk.start_time < prev_chunk.end_time:
                # overlap Ø¯Ø§Ø±Ø¯
                current_group.append(segment)
            else:
                # overlap Ù†Ø¯Ø§Ø±Ø¯ØŒ Ú¯Ø±ÙˆÙ‡ Ø¬Ø¯ÛŒØ¯
                groups.append(current_group)
                current_group = [segment]
                
        if current_group:
            groups.append(current_group)
            
        return groups
        
    async def _merge_overlapping_texts(
        self,
        segments: List[TranscriptSegment]
    ) -> str:
        """Ø§Ø¯ØºØ§Ù… Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ overlap"""
        
        if len(segments) != 2:
            # ÙØ¹Ù„Ø§Ù‹ ÙÙ‚Ø· Ø¯Ùˆ segment Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯
            return ' '.join([s.text for s in segments])
            
        text1 = segments[0].text
        text2 = segments[1].text
        
        # ÛŒØ§ÙØªÙ† Ø¨Ù‡ØªØ±ÛŒÙ† Ù†Ù‚Ø·Ù‡ ØªØ·Ø¨ÛŒÙ‚
        overlap_start = self._find_overlap_point(text1, text2)
        
        if overlap_start == -1:
            # ØªØ·Ø¨ÛŒÙ‚ ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ø³Ø§Ø¯Ù‡ ØªØ±Ú©ÛŒØ¨ Ú©Ù†
            return f"{text1} {text2}"
            
        # Ø§Ø¯ØºØ§Ù… Ø¨Ø¯ÙˆÙ† ØªÚ©Ø±Ø§Ø±
        merged = text1[:overlap_start] + text2
        
        # Ø­Ø°Ù Ú©Ù„Ù…Ø§Øª ØªÚ©Ø±Ø§Ø±ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ
        merged = self._remove_duplicate_words(merged)
        
        return merged
        
    def _find_overlap_point(self, text1: str, text2: str) -> int:
        """ÛŒØ§ÙØªÙ† Ù†Ù‚Ø·Ù‡ ØªØ·Ø¨ÛŒÙ‚ Ø¯Ùˆ Ù…ØªÙ†"""
        
        words1 = text1.split()
        words2 = text2.split()
        
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù¾Ù†Ø¬Ø±Ù‡ Ù„ØºØ²Ø§Ù†
        window_size = min(10, len(words2) // 2)
        
        for i in range(len(words1) - window_size, -1, -1):
            window1 = words1[i:i + window_size]
            
            # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ÛŒ text2
            if words2[:window_size] == window1:
                # ÛŒØ§ÙØª Ø´Ø¯
                return len(' '.join(words1[:i]))
                
        return -1  # ØªØ·Ø¨ÛŒÙ‚ ÛŒØ§ÙØª Ù†Ø´Ø¯
```

### Speaker Diarization

```python
# stt/services/speaker_diarization.py

class SpeakerDiarization:
    """ØªØ´Ø®ÛŒØµ Ùˆ ØªÙÚ©ÛŒÚ© Ú¯ÙˆÛŒÙ†Ø¯Ù‡â€ŒÙ‡Ø§"""
    
    def __init__(self):
        self.voice_analyzer = VoiceAnalyzer()
        self.speaker_classifier = SpeakerClassifier()
        
    async def diarize_transcript(
        self,
        audio_file_id: str,
        transcript: str
    ) -> Dict:
        """ØªØ´Ø®ÛŒØµ Ú¯ÙˆÛŒÙ†Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ"""
        
        audio_file = await AudioFile.objects.get(id=audio_file_id)
        
        # ØªØ­Ù„ÛŒÙ„ ØµÙˆØª Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ú¯ÙˆÛŒÙ†Ø¯Ù‡â€ŒÙ‡Ø§
        speakers = await self.voice_analyzer.detect_speakers(
            audio_file.storage_url
        )
        
        # ØªØ·Ø¨ÛŒÙ‚ Ù…ØªÙ† Ø¨Ø§ Ú¯ÙˆÛŒÙ†Ø¯Ù‡â€ŒÙ‡Ø§
        diarized_segments = []
        
        for segment in transcript.split('.'):
            if not segment.strip():
                continue
                
            # ØªØ´Ø®ÛŒØµ Ú¯ÙˆÛŒÙ†Ø¯Ù‡ Ø§ÛŒÙ† segment
            speaker = await self._identify_speaker(
                segment,
                speakers,
                audio_file
            )
            
            diarized_segments.append({
                'text': segment.strip() + '.',
                'speaker': speaker,
                'confidence': 0.85  # placeholder
            })
            
        # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ù‚Ø´â€ŒÙ‡Ø§ (Ù¾Ø²Ø´Ú©/Ø¨ÛŒÙ…Ø§Ø±)
        roles = await self._identify_roles(diarized_segments)
        
        return {
            'segments': diarized_segments,
            'speakers': speakers,
            'roles': roles,
            'formatted_transcript': self._format_diarized_transcript(
                diarized_segments,
                roles
            )
        }
        
    def _format_diarized_transcript(
        self,
        segments: List[Dict],
        roles: Dict
    ) -> str:
        """ÙØ±Ù…Øªâ€ŒØ¯Ù‡ÛŒ Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ Ø¨Ø§ Ú¯ÙˆÛŒÙ†Ø¯Ù‡â€ŒÙ‡Ø§"""
        
        formatted = []
        current_speaker = None
        
        for segment in segments:
            speaker_id = segment['speaker']
            role = roles.get(speaker_id, f'Ú¯ÙˆÛŒÙ†Ø¯Ù‡ {speaker_id}')
            
            if speaker_id != current_speaker:
                formatted.append(f"\n**{role}:** ")
                current_speaker = speaker_id
                
            formatted.append(segment['text'] + ' ')
            
        return ''.join(formatted).strip()
```

## âš¡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ú©ÛŒÙÛŒØª

### Audio Quality Enhancement

```python
# stt/processors/quality_enhancer.py
import numpy as np
from scipy import signal

class AudioQualityEnhancer:
    """Ø¨Ù‡Ø¨ÙˆØ¯ Ú©ÛŒÙÛŒØª ØµÙˆØª Ø¨Ø±Ø§ÛŒ STT"""
    
    def __init__(self):
        self.noise_reducer = NoiseReducer()
        self.normalizer = AudioNormalizer()
        self.enhancer = VoiceEnhancer()
        
    async def enhance_audio(
        self,
        audio_data: np.ndarray,
        sample_rate: int
    ) -> np.ndarray:
        """Ø¨Ù‡Ø¨ÙˆØ¯ Ú©ÛŒÙÛŒØª ØµÙˆØª"""
        
        # 1. Ú©Ø§Ù‡Ø´ Ù†ÙˆÛŒØ²
        denoised = await self.noise_reducer.reduce_noise(
            audio_data,
            sample_rate
        )
        
        # 2. Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø³Ø·Ø­ ØµØ¯Ø§
        normalized = await self.normalizer.normalize(
            denoised,
            target_db=-20
        )
        
        # 3. ØªÙ‚ÙˆÛŒØª ÙØ±Ú©Ø§Ù†Ø³â€ŒÙ‡Ø§ÛŒ ØµÙˆØª Ø§Ù†Ø³Ø§Ù†
        enhanced = await self.enhancer.enhance_voice(
            normalized,
            sample_rate
        )
        
        # 4. Ø­Ø°Ù Ø³Ú©ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ
        trimmed = await self._remove_silence(
            enhanced,
            sample_rate
        )
        
        return trimmed
        
    async def _remove_silence(
        self,
        audio: np.ndarray,
        sample_rate: int,
        threshold_db: float = -40
    ) -> np.ndarray:
        """Ø­Ø°Ù Ø³Ú©ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ"""
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ dB
        audio_db = 20 * np.log10(np.abs(audio) + 1e-10)
        
        # ÛŒØ§ÙØªÙ† Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø³Ú©ÙˆØª
        non_silent = audio_db > threshold_db
        
        # ÛŒØ§ÙØªÙ† Ø´Ø±ÙˆØ¹ Ùˆ Ù¾Ø§ÛŒØ§Ù†
        indices = np.where(non_silent)[0]
        if len(indices) == 0:
            return audio
            
        start = max(0, indices[0] - sample_rate // 10)  # 0.1s margin
        end = min(len(audio), indices[-1] + sample_rate // 10)
        
        return audio[start:end]
```

### STT Quality Metrics

```python
# stt/utils/metrics.py

class STTQualityMetrics:
    """Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ú©ÛŒÙÛŒØª STT"""
    
    def __init__(self):
        self.word_error_calculator = WordErrorRate()
        self.confidence_analyzer = ConfidenceAnalyzer()
        
    async def calculate_metrics(
        self,
        job_id: str
    ) -> Dict:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ú©ÛŒÙÛŒØª"""
        
        job = await STTJob.objects.get(id=job_id)
        segments = await TranscriptSegment.objects.filter(
            audio_chunk__audio_file__stt_job=job
        )
        
        # Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
        total_duration = sum(s.audio_chunk.duration for s in segments)
        total_words = sum(len(s.text.split()) for s in segments)
        avg_confidence = np.mean([s.confidence for s in segments])
        
        # ØªÙˆØ²ÛŒØ¹ confidence
        confidence_dist = {
            'high': len([s for s in segments if s.confidence > 0.9]),
            'medium': len([s for s in segments if 0.7 <= s.confidence <= 0.9]),
            'low': len([s for s in segments if s.confidence < 0.7])
        }
        
        # Ø³Ø±Ø¹Øª Ù¾Ø±Ø¯Ø§Ø²Ø´
        processing_time = (
            job.completed_at - job.created_at
        ).total_seconds()
        
        rtf = processing_time / total_duration  # Real-time factor
        
        metrics = {
            'total_duration_seconds': total_duration,
            'total_words': total_words,
            'words_per_minute': (total_words / total_duration) * 60,
            'average_confidence': avg_confidence,
            'confidence_distribution': confidence_dist,
            'processing_time_seconds': processing_time,
            'real_time_factor': rtf,
            'segments_count': len(segments),
            'success_rate': len([s for s in segments if s.audio_chunk.status == 'completed']) / len(segments)
        }
        
        return metrics
```

## ğŸ“Š Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ùˆ Ú¯Ø²Ø§Ø±Ø´â€ŒØ¯Ù‡ÛŒ

### STT Monitoring Service

```python
# stt/services/monitoring.py

class STTMonitoringService:
    """Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ø³ÛŒØ³ØªÙ… STT"""
    
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.alert_manager = AlertManager()
        
    async def collect_system_metrics(self) -> Dict:
        """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…"""
        
        now = timezone.now()
        hour_ago = now - timedelta(hours=1)
        
        # Ø¢Ù…Ø§Ø± jobs
        jobs_stats = await STTJob.objects.filter(
            created_at__gte=hour_ago
        ).aggregate(
            total=Count('id'),
            completed=Count('id', filter=Q(status='completed')),
            failed=Count('id', filter=Q(status='failed')),
            pending=Count('id', filter=Q(status='pending')),
            processing=Count('id', filter=Q(status='processing'))
        )
        
        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´
        completed_jobs = await STTJob.objects.filter(
            status='completed',
            created_at__gte=hour_ago
        ).annotate(
            processing_duration=F('completed_at') - F('created_at')
        )
        
        avg_processing_time = np.mean([
            j.processing_duration.total_seconds()
            for j in completed_jobs
        ]) if completed_jobs else 0
        
        # Ù†Ø±Ø® Ø®Ø·Ø§
        error_rate = (
            jobs_stats['failed'] / jobs_stats['total']
        ) if jobs_stats['total'] > 0 else 0
        
        # ÙˆØ¶Ø¹ÛŒØª ØµÙâ€ŒÙ‡Ø§
        queue_stats = await self._get_queue_stats()
        
        # API usage
        api_usage = await self._get_api_usage_stats()
        
        metrics = {
            'timestamp': now.isoformat(),
            'jobs': jobs_stats,
            'average_processing_time': avg_processing_time,
            'error_rate': error_rate,
            'queue_stats': queue_stats,
            'api_usage': api_usage,
            'health_status': self._calculate_health_status(
                error_rate,
                queue_stats
            )
        }
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§
        await self._check_alerts(metrics)
        
        return metrics
        
    async def _check_alerts(self, metrics: Dict):
        """Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø§Ø±Ø³Ø§Ù„ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§"""
        
        # Ù‡Ø´Ø¯Ø§Ø± Ù†Ø±Ø® Ø®Ø·Ø§ÛŒ Ø¨Ø§Ù„Ø§
        if metrics['error_rate'] > 0.1:  # 10%
            await self.alert_manager.send_alert(
                level='critical',
                title='Ù†Ø±Ø® Ø®Ø·Ø§ÛŒ STT Ø¨Ø§Ù„Ø§',
                message=f"Ù†Ø±Ø® Ø®Ø·Ø§: {metrics['error_rate']*100:.1f}%"
            )
            
        # Ù‡Ø´Ø¯Ø§Ø± ØµÙ Ø·ÙˆÙ„Ø§Ù†ÛŒ
        if metrics['queue_stats']['pending'] > 100:
            await self.alert_manager.send_alert(
                level='warning',
                title='ØµÙ STT Ø·ÙˆÙ„Ø§Ù†ÛŒ',
                message=f"ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø± ØµÙ: {metrics['queue_stats']['pending']}"
            )
```

### STT Analytics Dashboard

```python
# stt/api/views.py

class STTAnalyticsViewSet(viewsets.ViewSet):
    """API ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ STT"""
    
    permission_classes = [IsAuthenticated, IsAdminUser]
    
    @action(detail=False, methods=['get'])
    async def dashboard(self, request):
        """Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ ØªØ­Ù„ÛŒÙ„ÛŒ STT"""
        
        # Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ
        days = int(request.query_params.get('days', 7))
        start_date = timezone.now() - timedelta(days=days)
        
        # Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
        overview = await STTJob.objects.filter(
            created_at__gte=start_date
        ).aggregate(
            total_jobs=Count('id'),
            total_duration=Sum('audio_file__duration_seconds'),
            avg_confidence=Avg('metadata__average_confidence'),
            success_rate=Count('id', filter=Q(status='completed')) * 100.0 / Count('id')
        )
        
        # Ø±ÙˆÙ†Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡
        daily_trend = await STTJob.objects.filter(
            created_at__gte=start_date
        ).annotate(
            date=TruncDate('created_at')
        ).values('date').annotate(
            jobs=Count('id'),
            duration=Sum('audio_file__duration_seconds'),
            errors=Count('id', filter=Q(status='failed'))
        ).order_by('date')
        
        # ØªÙˆØ²ÛŒØ¹ Ø²Ø¨Ø§Ù†
        language_dist = await STTJob.objects.filter(
            created_at__gte=start_date
        ).values('metadata__language').annotate(
            count=Count('id')
        ).order_by('-count')
        
        # Ú©ÛŒÙÛŒØª Ø¨Ø± Ø§Ø³Ø§Ø³ encounter type
        quality_by_type = await self._get_quality_by_encounter_type(
            start_date
        )
        
        # Top errors
        top_errors = await self._get_top_errors(start_date)
        
        return Response({
            'overview': overview,
            'daily_trend': list(daily_trend),
            'language_distribution': list(language_dist),
            'quality_by_encounter_type': quality_by_type,
            'top_errors': top_errors,
            'period': {
                'start': start_date.isoformat(),
                'end': timezone.now().isoformat(),
                'days': days
            }
        })
```

---

[ELEMENT: div align="center"]

[â†’ Ù‚Ø¨Ù„ÛŒ: Ú†Øªâ€ŒØ¨Ø§Øª Ù¾Ø²Ø´Ú©ÛŒ](10-chatbot-system.md) | [Ø¨Ø¹Ø¯ÛŒ: ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ â†](12-output-generation.md)

</div>
