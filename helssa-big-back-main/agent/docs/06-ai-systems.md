# ๐ค ุณุณุชูโูุง ููุด ูุตููุน HELSSA

## ๐ ููุฑุณุช ูุทุงูุจ

- [ูุนุฑู ุณุณุชู AI](## ๐ฏ ูุนุฑู ุณุณุชู AI)
- [ูุนูุงุฑ ุณุณุชู ููุด ูุตููุน](## ๐๏ธ ูุนูุงุฑ ุณุณุชู ููุด ูุตููุน)
- [ฺุชโุจุงุช ูพุฒุดฺฉ](## ๐ฌ ฺุชโุจุงุช ูพุฒุดฺฉ)
- [ูพุฑุฏุงุฒุด ุตูุช (STT)](## ๐๏ธ ูพุฑุฏุงุฒุด ุตูุช (STT))
- [ูพุฑุฏุงุฒุด ุฒุจุงู ุทุจุน (NLP)](## ๐ ูพุฑุฏุงุฒุด ุฒุจุงู ุทุจุน (NLP))
- [ุชุญูู ุชุตุงูุฑ ูพุฒุดฺฉ](## ๐ผ๏ธ ุชุญูู ุชุตุงูุฑ ูพุฒุดฺฉ)
- [ูุฏุฑุช ูุญุฏูุฏุชโูุง](## ๐ ูุฏุฑุช ูุญุฏูุฏุชโูุง)
- [ุจูููโุณุงุฒ ู ฺฉุด](## ๐ ุจูููโุณุงุฒ ู ฺฉุด)

---

## ๐ฏ ูุนุฑู ุณุณุชู AI

ุณุณุชู ููุด ูุตููุน HELSSA ฺฉ ูพูุชูุฑู ุฌุงูุน ุงุณุช ฺฉู ุงุฒ ุฌุฏุฏุชุฑู ูุฏูโูุง AI ุจุฑุง ุงุฑุงุฆู ุฎุฏูุงุช ูพุฒุดฺฉ ููุดููุฏ ุงุณุชูุงุฏู ูโฺฉูุฏ.

### ูุงุจูุชโูุง ฺฉูุฏ

- ๐ฌ **ฺุชโุจุงุช ูพุฒุดฺฉ 24/7** ุจุง GPT-4
- ๐๏ธ **ุชุจุฏู ฺฏูุชุงุฑ ุจู ูุชู** ุจุง Whisper
- ๐ **ุชููุฏ ฺฏุฒุงุฑุด SOAP** ููุดููุฏ
- ๐ผ๏ธ **ุชุญูู ุชุตุงูุฑ ูพุฒุดฺฉ** ุจุง Vision Models
- ๐ง **ูพุฑุฏุงุฒุด ุฒุจุงู ุทุจุน** ูพุดุฑูุชู
- ๐ **ุชุญูู ุงุญุณุงุณุงุช** ุจูุงุฑุงู

## ๐๏ธ ูุนูุงุฑ ุณุณุชู ููุด ูุตููุน

```mermaid
graph TB
    subgraph "User Interface Layer"
        CHAT[Chat Interface]
        VOICE[Voice Input]
        IMG[Image Upload]
        TEXT[Text Input]
    end
    
    subgraph "API Gateway"
        GW[API Gateway]
        RL[Rate Limiter]
        AUTH[Auth Check]
    end
    
    subgraph "AI Service Layer"
        AIO[AI Orchestrator]
        CBS[Chatbot Service]
        STT[STT Service]
        NLP[NLP Service]
        VIS[Vision Service]
    end
    
    subgraph "Model Management"
        MM[Model Manager]
        LB[Load Balancer]
        FB[Fallback Logic]
    end
    
    subgraph "AI Providers"
        GPT[OpenAI/GapGPT]
        WHISPER[Whisper API]
        CLAUDE[Claude API]
        CUSTOM[Custom Models]
    end
    
    subgraph "Storage & Cache"
        REDIS[(Redis Cache)]
        MINIO[(MinIO Storage)]
        DB[(MySQL DB)]
    end
    
    CHAT --> GW
    VOICE --> GW
    IMG --> GW
    TEXT --> GW
    
    GW --> RL
    RL --> AUTH
    AUTH --> AIO
    
    AIO --> CBS
    AIO --> STT
    AIO --> NLP
    AIO --> VIS
    
    CBS --> MM
    STT --> MM
    NLP --> MM
    VIS --> MM
    
    MM --> LB
    LB --> GPT
    LB --> WHISPER
    LB --> CLAUDE
    LB --> CUSTOM
    
    MM --> FB
    
    AIO --> REDIS
    STT --> MINIO
    VIS --> MINIO
    CBS --> DB
```

### ุณุงุฎุชุงุฑ ูพุฑูฺู AI

```python
unified_ai/
โโโ __init__.py
โโโ apps.py                      # Django app config
โโโ models.py                    # ูุฏูโูุง AI
โโโ serializers.py               # ุณุฑุงูุงุฒุฑูุง
โโโ views/
โ   โโโ chat_views.py           # Chat API endpoints
โ   โโโ stt_views.py            # STT endpoints
โ   โโโ nlp_views.py            # NLP endpoints
โ   โโโ vision_views.py         # Vision endpoints
โโโ services/
โ   โโโ ai_orchestrator.py      # ููุงููฺฏโฺฉููุฏู ูุฑฺฉุฒ
โ   โโโ chatbot_service.py      # ุณุฑูุณ ฺุชโุจุงุช
โ   โโโ stt_service.py          # ุณุฑูุณ STT
โ   โโโ nlp_service.py          # ุณุฑูุณ NLP
โ   โโโ vision_service.py       # ุณุฑูุณ ุชุตูุฑ
โ   โโโ model_manager.py        # ูุฏุฑุช ูุฏูโูุง
โโโ providers/
โ   โโโ openai_provider.py      # OpenAI/GapGPT
โ   โโโ whisper_provider.py     # Whisper
โ   โโโ claude_provider.py      # Claude
โ   โโโ custom_provider.py      # Custom models
โโโ prompts/
โ   โโโ medical_prompts.py      # ูพุฑุงููพุชโูุง ูพุฒุดฺฉ
โ   โโโ soap_prompts.py         # ูพุฑุงููพุชโูุง SOAP
โ   โโโ chat_prompts.py         # ูพุฑุงููพุชโูุง ฺุช
โโโ utils/
โ   โโโ token_counter.py        # ุดูุงุฑุด ุชูฺฉู
โ   โโโ validators.py           # ุงุนุชุจุงุฑุณูุฌ
โ   โโโ formatters.py           # ูุฑูุชโฺฉููุฏูโูุง
โโโ middleware/
โ   โโโ rate_limiter.py         # ูุญุฏูุฏุช ูุฑุฎ
โ   โโโ usage_tracker.py        # ุฑุฏุงุจ ูุตุฑู
โโโ tasks.py                     # Celery tasks
```

## ๐ฌ ฺุชโุจุงุช ูพุฒุดฺฉ

### Medical Chatbot Service

```python
# unified_ai/services/chatbot_service.py
from typing import Dict, List, Optional, AsyncGenerator
import asyncio
from openai import AsyncOpenAI
from langchain.memory import ConversationSummaryBufferMemory
from langchain.schema import BaseMessage, HumanMessage, AIMessage

class MedicalChatbotService:
    """ุณุฑูุณ ฺุชโุจุงุช ูพุฒุดฺฉ ููุดููุฏ"""
    
    def __init__(self):
        self.client = AsyncOpenAI(
            api_key=settings.OPENAI_API_KEY,
            base_url=settings.OPENAI_BASE_URL  # ูุงุจู ุชูุธู ุจุฑุง GapGPT
        )
        self.model = "gpt-4-turbo-preview"
        self.memory_manager = ConversationMemoryManager()
        self.medical_validator = MedicalContentValidator()
        
    async def process_message(
        self,
        user_id: str,
        message: str,
        conversation_id: Optional[str] = None,
        context: Optional[Dict] = None
    ) -> Dict:
        """ูพุฑุฏุงุฒุด ูพุงู ฺฉุงุฑุจุฑ"""
        
        # ุจุงุฒุงุจ ุง ุงุฌุงุฏ ูฺฉุงููู
        conversation = await self._get_or_create_conversation(
            user_id, conversation_id
        )
        
        # ุงุนุชุจุงุฑุณูุฌ ูุญุชูุง ูพุฒุดฺฉ
        validation = await self.medical_validator.validate(message)
        if validation['contains_emergency']:
            return await self._handle_emergency(message, validation)
            
        # ุจุงุฒุงุจ ุชุงุฑุฎฺู
        history = await self.memory_manager.get_conversation_history(
            conversation.id
        )
        
        # ุขูุงุฏูโุณุงุฒ ูพุฑุงููพุช
        messages = self._prepare_messages(message, history, context)
        
        try:
            # ูุฑุงุฎูุงู ูุฏู
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                max_tokens=2000,
                stream=False
            )
            
            ai_response = response.choices[0].message.content
            
            # ุฐุฎุฑู ุฏุฑ ุชุงุฑุฎฺู
            await self._save_to_history(
                conversation.id,
                message,
                ai_response
            )
            
            # ุชุญูู ูพุงุณุฎ
            analysis = await self._analyze_response(ai_response)
            
            return {
                'conversation_id': str(conversation.id),
                'response': ai_response,
                'analysis': analysis,
                'suggestions': await self._get_suggestions(analysis),
                'tokens_used': response.usage.total_tokens
            }
            
        except Exception as e:
            # Fallback strategy
            return await self._fallback_response(message, str(e))
            
    async def stream_response(
        self,
        user_id: str,
        message: str,
        conversation_id: str
    ) -> AsyncGenerator[str, None]:
        """ูพุงุณุฎ ุงุณุชุฑููฺฏ ุจุฑุง ุชุฌุฑุจู ุจูุชุฑ"""
        
        conversation = await self._get_conversation(conversation_id)
        history = await self.memory_manager.get_conversation_history(
            conversation.id
        )
        
        messages = self._prepare_messages(message, history)
        
        stream = await self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0.7,
            max_tokens=2000,
            stream=True
        )
        
        full_response = ""
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_response += content
                yield content
                
        # ุฐุฎุฑู ูพุงุณุฎ ฺฉุงูู
        await self._save_to_history(
            conversation.id,
            message,
            full_response
        )
        
    def _prepare_messages(
        self,
        message: str,
        history: List[BaseMessage],
        context: Optional[Dict] = None
    ) -> List[Dict]:
        """ุขูุงุฏูโุณุงุฒ ูพุงูโูุง ุจุฑุง ุงุฑุณุงู ุจู ูุฏู"""
        
        messages = [{
            "role": "system",
            "content": self._get_system_prompt(context)
        }]
        
        # ุงูุฒูุฏู ุชุงุฑุฎฺู (ุญุฏุงฺฉุซุฑ 10 ูพุงู ุงุฎุฑ)
        for msg in history[-10:]:
            if isinstance(msg, HumanMessage):
                messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                messages.append({"role": "assistant", "content": msg.content})
                
        # ูพุงู ุฌุฏุฏ
        messages.append({"role": "user", "content": message})
        
        return messages
        
    def _get_system_prompt(self, context: Optional[Dict] = None) -> str:
        """ูพุฑุงููพุช ุณุณุชู ุจุฑุง ฺุชโุจุงุช ูพุฒุดฺฉ"""
        
        base_prompt = """ุดูุง ฺฉ ุฏุณุชุงุฑ ูพุฒุดฺฉ ููุดููุฏ ุฏุฑ ูพูุชูุฑู HELSSA ูุณุชุฏ. ูุธุงู ุดูุง:

1. ูพุงุณุฎ ุจู ุณูุงูุงุช ูพุฒุดฺฉ ุจุง ุฏูุช ู ุงุญุชุงุท
2. ุงุฑุงุฆู ุงุทูุงุนุงุช ูุงุจู ููู ู ููุฏ
3. ุชุงฺฉุฏ ุจุฑ ุงูฺฉู ุงู ูุดุงูุฑู ุฌุงฺฏุฒู ูุฒุช ุญุถูุฑ ูุณุช
4. ุดูุงุณุง ููุงุฑุฏ ุงูุฑฺุงูุณ ู ุงุฑุฌุงุน ููุฑ
5. ุญูุธ ุญุฑู ุฎุตูุต ู ุฑุงุฒุฏุงุฑ ูพุฒุดฺฉ

ูุญุฏูุฏุชโูุง:
- ุชุฌูุฒ ุฏุงุฑู ููููุน ุงุณุช
- ุชุดุฎุต ูุทุน ููููุน ุงุณุช
- ููุดู ุชูุตู ุจู ูุดุงูุฑู ุญุถูุฑ ุฏุฑ ููุงุฑุฏ ุฌุฏ

ุฒุจุงู: ูุงุฑุณ (ูฺฏุฑ ุงูฺฉู ฺฉุงุฑุจุฑ ุจู ุฒุจุงู ุฏฺฏุฑ ุตุญุจุช ฺฉูุฏ)"""
        
        if context:
            if context.get('patient_age'):
                base_prompt += f"\n\nุณู ุจูุงุฑ: {context['patient_age']} ุณุงู"
            if context.get('medical_history'):
                base_prompt += f"\n\nุณุงุจูู ูพุฒุดฺฉ: {context['medical_history']}"
                
        return base_prompt
```

### Conversation Management

```python
# unified_ai/services/conversation_manager.py

class ConversationMemoryManager:
    """ูุฏุฑุช ุญุงูุธู ูฺฉุงููุงุช"""
    
    def __init__(self):
        self.redis_client = get_redis_client()
        self.summary_threshold = 20  # ุชุนุฏุงุฏ ูพุงู ุจุฑุง ุฎูุงุตูโุณุงุฒ
        
    async def get_conversation_history(
        self,
        conversation_id: str
    ) -> List[BaseMessage]:
        """ุจุงุฒุงุจ ุชุงุฑุฎฺู ูฺฉุงููู"""
        
        # ุจุงุฒุงุจ ุงุฒ ุฏุชุงุจุณ
        messages = await ChatMessage.objects.filter(
            conversation_id=conversation_id
        ).order_by('created_at').all()
        
        history = []
        for msg in messages:
            if msg.role == 'user':
                history.append(HumanMessage(content=msg.content))
            else:
                history.append(AIMessage(content=msg.content))
                
        # ุงฺฏุฑ ุชุนุฏุงุฏ ูพุงูโูุง ุฒุงุฏ ุงุณุชุ ุฎูุงุตูโุณุงุฒ
        if len(history) > self.summary_threshold:
            history = await self._summarize_history(history)
            
        return history
        
    async def _summarize_history(
        self,
        history: List[BaseMessage]
    ) -> List[BaseMessage]:
        """ุฎูุงุตูโุณุงุฒ ุชุงุฑุฎฺู ุทููุงู"""
        
        # ูพุงูโูุง ุงููู ุฑุง ุฎูุงุตู ูโฺฉูู
        to_summarize = history[:-10]  # 10 ูพุงู ุขุฎุฑ ุฑุง ูฺฏู ูโุฏุงุฑู
        recent = history[-10:]
        
        # ุชุจุฏู ุจู ูุชู
        conversation_text = "\n".join([
            f"{'Human' if isinstance(msg, HumanMessage) else 'AI'}: {msg.content}"
            for msg in to_summarize
        ])
        
        # ุฎูุงุตูโุณุงุฒ ุจุง AI
        summary = await self._generate_summary(conversation_text)
        
        # ุชุฑฺฉุจ ุฎูุงุตู ุจุง ูพุงูโูุง ุงุฎุฑ
        return [
            AIMessage(content=f"ุฎูุงุตู ูฺฉุงููู ูุจู:\n{summary}")
        ] + recent
```

## ๐๏ธ ูพุฑุฏุงุฒุด ุตูุช (STT)

### Whisper STT Service

```python
# unified_ai/services/stt_service.py
import io
import asyncio
from pydub import AudioSegment
from typing import Dict, List, Optional

class WhisperSTTService:
    """ุณุฑูุณ ุชุจุฏู ฺฏูุชุงุฑ ุจู ูุชู ุจุง Whisper"""
    
    def __init__(self):
        self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        self.supported_formats = ['mp3', 'mp4', 'mpeg', 'mpga', 'm4a', 'wav', 'webm']
        self.max_file_size = 25 * 1024 * 1024  # 25MB
        self.chunk_duration = 60  # seconds
        
    async def transcribe_audio(
        self,
        audio_file: bytes,
        filename: str,
        language: str = 'fa',  # ูุงุฑุณ
        prompt: Optional[str] = None
    ) -> Dict:
        """ุชุจุฏู ูุงู ุตูุช ุจู ูุชู"""
        
        # ุงุนุชุจุงุฑุณูุฌ ูุงู
        self._validate_audio_file(audio_file, filename)
        
        # ุชุจุฏู ูุฑูุช ุฏุฑ ุตูุฑุช ูุงุฒ
        audio_file, format = await self._prepare_audio(audio_file, filename)
        
        # ุงฺฏุฑ ูุงู ุจุฒุฑฺฏ ุงุณุชุ ุชฺฉูโุชฺฉู ฺฉู
        if len(audio_file) > self.max_file_size:
            return await self._transcribe_large_file(
                audio_file, language, prompt
            )
            
        try:
            # ุงุฑุณุงู ุจู Whisper API
            response = await self.client.audio.transcriptions.create(
                model="whisper-1",
                file=("audio.mp3", audio_file, "audio/mpeg"),
                language=language,
                prompt=prompt or self._get_medical_prompt(),
                response_format="verbose_json"
            )
            
            # ูพุฑุฏุงุฒุด ูพุงุณุฎ
            return {
                'text': response.text,
                'language': response.language,
                'duration': response.duration,
                'segments': self._process_segments(response.segments),
                'confidence': self._calculate_confidence(response)
            }
            
        except Exception as e:
            # Fallback ุจู ุณุฑูุณ ุฏฺฏุฑ
            return await self._fallback_transcription(audio_file, language)
            
    async def _transcribe_large_file(
        self,
        audio_file: bytes,
        language: str,
        prompt: Optional[str]
    ) -> Dict:
        """ูพุฑุฏุงุฒุด ูุงูโูุง ุจุฒุฑฺฏ ุจุง ุชฺฉูโุจูุฏ"""
        
        # ุชุจุฏู ุจู AudioSegment
        audio = AudioSegment.from_file(io.BytesIO(audio_file))
        
        # ุชฺฉูโุจูุฏ
        chunks = self._split_audio(audio, self.chunk_duration)
        
        # ูพุฑุฏุงุฒุด ููุงุฒ ุชฺฉูโูุง
        tasks = []
        for i, chunk in enumerate(chunks):
            chunk_bytes = chunk.export(format="mp3").read()
            task = self._transcribe_chunk(
                chunk_bytes, i, language, prompt
            )
            tasks.append(task)
            
        # ุฌูุนโุขูุฑ ูุชุงุฌ
        results = await asyncio.gather(*tasks)
        
        # ุงุฏุบุงู ุฑูููุณโูุง
        return self._merge_transcriptions(results)
        
    def _split_audio(
        self,
        audio: AudioSegment,
        chunk_duration: int
    ) -> List[AudioSegment]:
        """ุชูุณู ูุงู ุตูุช ุจู ุชฺฉูโูุง ฺฉูฺฺฉุชุฑ"""
        
        chunks = []
        duration_ms = chunk_duration * 1000
        
        for i in range(0, len(audio), duration_ms):
            chunk = audio[i:i + duration_ms]
            chunks.append(chunk)
            
        return chunks
        
    async def _transcribe_chunk(
        self,
        chunk_bytes: bytes,
        chunk_index: int,
        language: str,
        prompt: Optional[str]
    ) -> Dict:
        """ุฑูููุณ ฺฉ ุชฺฉู ุตูุช"""
        
        try:
            response = await self.client.audio.transcriptions.create(
                model="whisper-1",
                file=(f"chunk_{chunk_index}.mp3", chunk_bytes, "audio/mpeg"),
                language=language,
                prompt=prompt
            )
            
            return {
                'index': chunk_index,
                'text': response.text,
                'success': True
            }
        except Exception as e:
            return {
                'index': chunk_index,
                'text': '',
                'success': False,
                'error': str(e)
            }
            
    def _merge_transcriptions(self, results: List[Dict]) -> Dict:
        """ุงุฏุบุงู ุฑูููุณ ุชฺฉูโูุง"""
        
        # ูุฑุชุจโุณุงุฒ ุจุฑ ุงุณุงุณ index
        results.sort(key=lambda x: x['index'])
        
        # ุงุฏุบุงู ูุชูโูุง
        full_text = " ".join([
            r['text'] for r in results if r['success']
        ])
        
        # ูุญุงุณุจู ุขูุงุฑ
        success_count = sum(1 for r in results if r['success'])
        total_count = len(results)
        
        return {
            'text': full_text,
            'chunks_processed': total_count,
            'chunks_successful': success_count,
            'success_rate': success_count / total_count if total_count > 0 else 0
        }
        
    def _get_medical_prompt(self) -> str:
        """ูพุฑุงููพุช ุชุฎุตุต ูพุฒุดฺฉ ุจุฑุง ุจูุจูุฏ ุฏูุช"""
        
        return """ุงู ฺฉ ูฺฉุงููู ูพุฒุดฺฉ ุจู ูพุฒุดฺฉ ู ุจูุงุฑ ุงุณุช.
        ุงุตุทูุงุญุงุช ูพุฒุดฺฉ ุฑุงุฌ: ูุฑุตุ ุขููพููุ ุขุฒูุงุดุ ูุนุงููุ ุชุดุฎุตุ ุฏุฑูุงูุ ูุณุฎู
        ุฏุงุฑููุง ุฑุงุฌ: ุงุณุชุงูููููุ ุขููฺฉุณโุณููุ ูุชููุฑููุ ููุฒุงุฑุชุงู
        ุจูุงุฑโูุง ุฑุงุฌ: ุฏุงุจุชุ ูุดุงุฑ ุฎููุ ุณุฑูุงุฎูุฑุฏฺฏุ ุขููููุงูุฒุง"""
```

### Audio Processing Pipeline

```python
# unified_ai/tasks.py
from celery import shared_task, chain, group
from typing import List, Dict

@shared_task(queue='stt')
def process_encounter_audio(encounter_id: str) -> Dict:
    """ูพุฑุฏุงุฒุด ฺฉุงูู ุตูุช ฺฉ ููุงูุงุช"""
    
    # ุจุงุฒุงุจ encounter ู ูุงูโูุง ุตูุช
    encounter = Encounter.objects.get(id=encounter_id)
    audio_chunks = AudioChunk.objects.filter(
        encounter=encounter
    ).order_by('chunk_index')
    
    # ุงุฌุงุฏ ุฒูุฌุฑู ูุธุงู
    workflow = chain(
        # 1. ุฑูููุณ ููู chunk ูุง
        group([
            transcribe_audio_chunk.s(chunk.id)
            for chunk in audio_chunks
        ]),
        
        # 2. ุงุฏุบุงู ุฑูููุณโูุง
        merge_transcriptions.s(encounter_id),
        
        # 3. ูพุฑุฏุงุฒุด NLP
        extract_medical_entities.s(),
        
        # 4. ุชููุฏ ฺฏุฒุงุฑุด SOAP
        generate_soap_report.s(encounter_id),
        
        # 5. ุงุฑุณุงู ููุชูฺฉุดู
        notify_doctor_report_ready.s()
    )
    
    # ุงุฌุฑุง workflow
    result = workflow.apply_async()
    
    return {
        'encounter_id': encounter_id,
        'workflow_id': result.id,
        'status': 'processing'
    }

@shared_task
def transcribe_audio_chunk(chunk_id: str) -> Dict:
    """ุฑูููุณ ฺฉ chunk ุตูุช"""
    
    chunk = AudioChunk.objects.get(id=chunk_id)
    
    # ุฏุงูููุฏ ุงุฒ MinIO
    audio_data = download_from_minio(chunk.file_url)
    
    # ุฑูููุณ
    stt_service = WhisperSTTService()
    result = asyncio.run(
        stt_service.transcribe_audio(
            audio_data,
            f"chunk_{chunk.chunk_index}.mp3"
        )
    )
    
    # ุฐุฎุฑู ูุชุฌู
    Transcript.objects.create(
        audio_chunk=chunk,
        text=result['text'],
        language=result.get('language', 'fa'),
        confidence=result.get('confidence', 0.0),
        metadata=result
    )
    
    return {
        'chunk_id': chunk_id,
        'text_length': len(result['text']),
        'success': True
    }
```

## ๐ ูพุฑุฏุงุฒุด ุฒุจุงู ุทุจุน (NLP)

### Medical NLP Service

```python
# unified_ai/services/nlp_service.py
import re
from typing import Dict, List, Optional
import spacy
from hazm import Normalizer, SentenceTokenizer

class MedicalNLPService:
    """ุณุฑูุณ ูพุฑุฏุงุฒุด ุฒุจุงู ุทุจุน ูพุฒุดฺฉ"""
    
    def __init__(self):
        self.normalizer = Normalizer()
        self.sentence_tokenizer = SentenceTokenizer()
        self.entity_extractor = MedicalEntityExtractor()
        self.soap_generator = SOAPGenerator()
        
    async def extract_medical_entities(
        self,
        text: str,
        language: str = 'fa'
    ) -> Dict:
        """ุงุณุชุฎุฑุงุฌ ููุฌูุฏุชโูุง ูพุฒุดฺฉ ุงุฒ ูุชู"""
        
        # ูุฑูุงูโุณุงุฒ ูุชู
        normalized_text = self.normalizer.normalize(text)
        
        # ุงุณุชุฎุฑุงุฌ ููุฌูุฏุชโูุง
        entities = {
            'symptoms': await self._extract_symptoms(normalized_text),
            'medications': await self._extract_medications(normalized_text),
            'diagnoses': await self._extract_diagnoses(normalized_text),
            'lab_tests': await self._extract_lab_tests(normalized_text),
            'vital_signs': await self._extract_vital_signs(normalized_text),
            'allergies': await self._extract_allergies(normalized_text),
            'medical_history': await self._extract_medical_history(normalized_text)
        }
        
        # ุชุญูู ุงุญุณุงุณุงุช ุจูุงุฑ
        sentiment = await self._analyze_patient_sentiment(normalized_text)
        
        return {
            'entities': entities,
            'sentiment': sentiment,
            'summary': await self._generate_summary(entities),
            'risk_factors': await self._identify_risk_factors(entities)
        }
        
    async def generate_soap_report(
        self,
        transcript: str,
        chief_complaint: str,
        patient_info: Optional[Dict] = None
    ) -> Dict:
        """ุชููุฏ ฺฏุฒุงุฑุด SOAP ุงุฒ ุฑูููุณ"""
        
        # ุงุณุชุฎุฑุงุฌ ููุฌูุฏุชโูุง
        entities = await self.extract_medical_entities(transcript)
        
        # ุขูุงุฏูโุณุงุฒ context
        context = {
            'transcript': transcript,
            'chief_complaint': chief_complaint,
            'entities': entities['entities'],
            'patient_info': patient_info or {}
        }
        
        # ุชููุฏ ุจุฎุดโูุง SOAP
        soap_sections = await self.soap_generator.generate_all_sections(context)
        
        # ุงุนุชุจุงุฑุณูุฌ ู ุชฺฉูู
        validated_soap = await self._validate_soap_sections(soap_sections)
        
        return {
            'subjective': validated_soap['subjective'],
            'objective': validated_soap['objective'],
            'assessment': validated_soap['assessment'],
            'plan': validated_soap['plan'],
            'metadata': {
                'generated_at': datetime.utcnow().isoformat(),
                'confidence_score': validated_soap['confidence'],
                'entities_found': len(entities['entities']),
                'warnings': validated_soap.get('warnings', [])
            }
        }
        
    async def _extract_symptoms(self, text: str) -> List[Dict]:
        """ุงุณุชุฎุฑุงุฌ ุนูุงุฆู ุจูุงุฑ"""
        
        symptom_patterns = [
            r'ุฏุฑุฏ\s+(\w+)',
            r'(\w+)\s+ุฏุฑุฏ',
            r'ุงุญุณุงุณ\s+(\w+)',
            r'ุฏฺุงุฑ\s+(\w+)',
            r'ุนูุงุฆู?\s+(\w+)',
        ]
        
        symptoms = []
        for pattern in symptom_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                symptom = {
                    'name': match,
                    'severity': self._extract_severity(text, match),
                    'duration': self._extract_duration(text, match),
                    'location': self._extract_location(text, match)
                }
                symptoms.append(symptom)
                
        return self._deduplicate_entities(symptoms)
        
    async def _extract_medications(self, text: str) -> List[Dict]:
        """ุงุณุชุฎุฑุงุฌ ุฏุงุฑููุง"""
        
        # ูุณุช ุฏุงุฑููุง ุฑุงุฌ
        common_drugs = [
            'ุงุณุชุงููููู', 'ุขููฺฉุณโุณูู', 'ูุชููุฑูู', 'ููุฒุงุฑุชุงู',
            'ุขุณูพุฑู', 'ุงุจููพุฑููู', 'ูพุฑุฏูุฒูููู', 'ุขุชูุฑูุงุณุชุงุชู'
        ]
        
        medications = []
        for drug in common_drugs:
            if drug in text:
                # ุงุณุชุฎุฑุงุฌ ุฏูุฒ ู ุฏุณุชูุฑ ูุตุฑู
                dosage = self._extract_dosage(text, drug)
                frequency = self._extract_frequency(text, drug)
                
                medications.append({
                    'name': drug,
                    'dosage': dosage,
                    'frequency': frequency,
                    'route': self._extract_route(text, drug)
                })
                
        return medications
```

### SOAP Generator

```python
# unified_ai/services/soap_generator.py

class SOAPGenerator:
    """ุชููุฏฺฉููุฏู ฺฏุฒุงุฑุด SOAP"""
    
    def __init__(self):
        self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        self.validator = SOAPValidator()
        
    async def generate_all_sections(self, context: Dict) -> Dict:
        """ุชููุฏ ุชูุงู ุจุฎุดโูุง SOAP"""
        
        tasks = {
            'subjective': self._generate_subjective(context),
            'objective': self._generate_objective(context),
            'assessment': self._generate_assessment(context),
            'plan': self._generate_plan(context)
        }
        
        # ุงุฌุฑุง ููุงุฒ
        results = await asyncio.gather(*tasks.values())
        
        return dict(zip(tasks.keys(), results))
        
    async def _generate_subjective(self, context: Dict) -> str:
        """ุชููุฏ ุจุฎุด Subjective"""
        
        prompt = f"""ุจุฑ ุงุณุงุณ ุฑูููุณ ุฒุฑุ ุจุฎุด Subjective ฺฏุฒุงุฑุด SOAP ุฑุง ุจููุณุฏ.

ุดฺฉุงุช ุงุตู: {context['chief_complaint']}

ุฑูููุณ ูฺฉุงููู:
{context['transcript']}

ุจุฎุด Subjective ุจุงุฏ ุดุงูู:
1. ุดฺฉุงุช ุงุตู (CC)
2. ุชุงุฑุฎฺู ุจูุงุฑ ูุนู (HPI)
3. ุณุงุจูู ูพุฒุดฺฉ ฺฏุฐุดุชู (PMH)
4. ุฏุงุฑููุง ูุตุฑู ูุนู
5. ุขูุฑฺโูุง
6. ุณุงุจูู ุงุฌุชูุงุน ูุฑุชุจุท

ููุท ุจุฎุด Subjective ุฑุง ุจููุณุฏุ ุจู ูุงุฑุณ ู ุจุตูุฑุช ุญุฑููโุง."""

        response = await self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ุดูุง ฺฉ ูพุฒุดฺฉ ูุชุฎุตุต ูุณุชุฏ ฺฉู ฺฏุฒุงุฑุดโูุง SOAP ูโููุณุฏ."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )
        
        return response.choices[0].message.content
        
    async def _generate_objective(self, context: Dict) -> str:
        """ุชููุฏ ุจุฎุด Objective"""
        
        entities = context['entities']
        
        prompt = f"""ุจุฎุด Objective ฺฏุฒุงุฑุด SOAP ุฑุง ุจุฑ ุงุณุงุณ ุงุทูุงุนุงุช ุฒุฑ ุจููุณุฏ:

ุนูุงุฆู ุญุงุช: {entities.get('vital_signs', [])}
ุงูุชูโูุง ูุนุงูู: {self._extract_examination_findings(context['transcript'])}
ูุชุงุฌ ุขุฒูุงุดุงุช: {entities.get('lab_tests', [])}

ุจุฎุด Objective ุจุงุฏ ุดุงูู:
1. ุนูุงุฆู ุญุงุช (VS)
2. ุงูุชูโูุง ูุนุงูู ูุฒฺฉ
3. ูุชุงุฌ ุขุฒูุงุดุงุช ู ุชุตูุฑุจุฑุฏุงุฑ
4. ูุดุงูุฏุงุช ุนู

ุจู ูุงุฑุณ ู ููุท ุจุฎุด Objective ุฑุง ุจููุณุฏ."""

        response = await self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ุดูุง ฺฉ ูพุฒุดฺฉ ูุชุฎุตุต ูุณุชุฏ."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=800
        )
        
        return response.choices[0].message.content
```

## ๐ผ๏ธ ุชุญูู ุชุตุงูุฑ ูพุฒุดฺฉ

### Medical Vision Service

```python
# unified_ai/services/vision_service.py
from PIL import Image
import numpy as np
from typing import Dict, List, Optional

class MedicalVisionService:
    """ุณุฑูุณ ุชุญูู ุชุตุงูุฑ ูพุฒุดฺฉ"""
    
    def __init__(self):
        self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        self.supported_types = [
            'xray', 'mri', 'ct_scan', 'ultrasound',
            'ecg', 'lab_report', 'prescription'
        ]
        
    async def analyze_medical_image(
        self,
        image_data: bytes,
        image_type: str,
        additional_context: Optional[str] = None
    ) -> Dict:
        """ุชุญูู ุชุตูุฑ ูพุฒุดฺฉ"""
        
        # ุงุนุชุจุงุฑุณูุฌ ููุน ุชุตูุฑ
        if image_type not in self.supported_types:
            raise ValueError(f"Unsupported image type: {image_type}")
            
        # ูพุดโูพุฑุฏุงุฒุด ุชุตูุฑ
        processed_image = await self._preprocess_image(image_data, image_type)
        
        # ุชุญูู ุจุง Vision API
        if image_type in ['xray', 'mri', 'ct_scan']:
            return await self._analyze_radiology_image(
                processed_image, image_type, additional_context
            )
        elif image_type == 'lab_report':
            return await self._extract_lab_values(processed_image)
        elif image_type == 'prescription':
            return await self._read_prescription(processed_image)
        elif image_type == 'ecg':
            return await self._analyze_ecg(processed_image)
            
    async def _analyze_radiology_image(
        self,
        image_data: bytes,
        modality: str,
        context: Optional[str]
    ) -> Dict:
        """ุชุญูู ุชุตุงูุฑ ุฑุงุฏูููฺ"""
        
        prompt = self._get_radiology_prompt(modality, context)
        
        response = await self.client.chat.completions.create(
            model="gpt-4-vision-preview",
            messages=[
                {
                    "role": "system",
                    "content": "ุดูุง ฺฉ ุฑุงุฏูููฺุณุช ูุชุฎุตุต ูุณุชุฏ."
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{self._encode_image(image_data)}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=1500
        )
        
        analysis = response.choices[0].message.content
        
        # ุงุณุชุฎุฑุงุฌ ุงูุชูโูุง ฺฉูุฏ
        findings = await self._extract_key_findings(analysis)
        
        return {
            'modality': modality,
            'analysis': analysis,
            'key_findings': findings,
            'recommendations': await self._generate_recommendations(findings),
            'confidence': self._calculate_confidence(analysis)
        }
        
    async def _extract_lab_values(self, image_data: bytes) -> Dict:
        """ุงุณุชุฎุฑุงุฌ ููุงุฏุฑ ุขุฒูุงุดฺฏุงู ุงุฒ ุชุตูุฑ"""
        
        # OCR ุจุฑุง ุงุณุชุฎุฑุงุฌ ูุชู
        text = await self._perform_ocr(image_data)
        
        # ุชุญูู ุจุง GPT-4
        prompt = f"""ูุชู ุฒุฑ ุงุฒ ฺฉ ุจุฑฺฏู ุขุฒูุงุด ุงุณุชุฎุฑุงุฌ ุดุฏู ุงุณุช.
ูุทูุงู ููุงุฏุฑ ุขุฒูุงุดฺฏุงู ุฑุง ุงุณุชุฎุฑุงุฌ ู ุฏุณุชูโุจูุฏ ฺฉูุฏ:

{text}

ุฎุฑูุฌ ุฑุง ุจู ุตูุฑุช JSON ุจุง ุงู ุณุงุฎุชุงุฑ ุงุฑุงุฆู ุฏูุฏ:
{{
    "test_name": "ูุงู ุขุฒูุงุด",
    "value": "ููุฏุงุฑ",
    "unit": "ูุงุญุฏ",
    "reference_range": "ูุญุฏูุฏู ูุฑูุงู",
    "status": "normal/high/low"
}}"""

        response = await self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ุดูุง ุฏุฑ ุงุณุชุฎุฑุงุฌ ุฏุงุฏูโูุง ุขุฒูุงุดฺฏุงู ุชุฎุตุต ุฏุงุฑุฏ."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            response_format={"type": "json_object"}
        )
        
        lab_values = json.loads(response.choices[0].message.content)
        
        # ุชุญูู ููุงุฏุฑ ุบุฑุทุจุน
        abnormal_values = [
            v for v in lab_values
            if v.get('status') in ['high', 'low']
        ]
        
        return {
            'lab_values': lab_values,
            'abnormal_count': len(abnormal_values),
            'abnormal_values': abnormal_values,
            'clinical_significance': await self._assess_clinical_significance(
                abnormal_values
            )
        }
```

## ๐ ูุฏุฑุช ูุญุฏูุฏุชโูุง

### Usage Management

```python
# unified_ai/services/usage_manager.py

class AIUsageManager:
    """ูุฏุฑุช ูุตุฑู ู ูุญุฏูุฏุชโูุง AI"""
    
    def __init__(self):
        self.redis_client = get_redis_client()
        self.limits = self._load_limits()
        
    async def check_and_consume(
        self,
        user_id: str,
        service: str,
        tokens: int = 0
    ) -> bool:
        """ุจุฑุฑุณ ู ูุตุฑู ุณููู"""
        
        # ุจุงุฒุงุจ ูพูู ฺฉุงุฑุจุฑ
        user_plan = await self._get_user_plan(user_id)
        
        # ุจุฑุฑุณ ูุญุฏูุฏุชโูุง
        daily_limit = self.limits[user_plan][service]['daily']
        monthly_limit = self.limits[user_plan][service]['monthly']
        
        # ฺฉูุฏูุง Redis
        daily_key = f"usage:{user_id}:{service}:daily:{datetime.now().date()}"
        monthly_key = f"usage:{user_id}:{service}:monthly:{datetime.now().strftime('%Y-%m')}"
        
        # ุจุฑุฑุณ ูุตุฑู ูุนู
        daily_usage = int(await self.redis_client.get(daily_key) or 0)
        monthly_usage = int(await self.redis_client.get(monthly_key) or 0)
        
        # ุจุฑุฑุณ ูุญุฏูุฏุช
        if daily_usage + tokens > daily_limit:
            raise UsageLimitExceeded(
                f"ูุญุฏูุฏุช ุฑูุฒุงูู {service} ุชูุงู ุดุฏู ุงุณุช"
            )
            
        if monthly_usage + tokens > monthly_limit:
            raise UsageLimitExceeded(
                f"ูุญุฏูุฏุช ูุงูุงูู {service} ุชูุงู ุดุฏู ุงุณุช"
            )
            
        # ุซุจุช ูุตุฑู
        await self.redis_client.incrby(daily_key, tokens)
        await self.redis_client.expire(daily_key, 86400)  # 1 day
        
        await self.redis_client.incrby(monthly_key, tokens)
        await self.redis_client.expire(monthly_key, 2592000)  # 30 days
        
        # ุซุจุช ุฏุฑ ุฏุชุงุจุณ
        await self._log_usage(user_id, service, tokens)
        
        return True
        
    def _load_limits(self) -> Dict:
        """ุจุงุฑฺฏุฐุงุฑ ูุญุฏูุฏุชโูุง ูพููโูุง"""
        
        return {
            'free': {
                'chat': {'daily': 20, 'monthly': 200},
                'stt': {'daily': 10, 'monthly': 100},
                'vision': {'daily': 5, 'monthly': 50}
            },
            'basic': {
                'chat': {'daily': 100, 'monthly': 2000},
                'stt': {'daily': 50, 'monthly': 1000},
                'vision': {'daily': 20, 'monthly': 400}
            },
            'premium': {
                'chat': {'daily': 500, 'monthly': 15000},
                'stt': {'daily': 200, 'monthly': 6000},
                'vision': {'daily': 100, 'monthly': 3000}
            },
            'enterprise': {
                'chat': {'daily': -1, 'monthly': -1},  # ูุงูุญุฏูุฏ
                'stt': {'daily': -1, 'monthly': -1},
                'vision': {'daily': -1, 'monthly': -1}
            }
        }
```

## ๐ ุจูููโุณุงุฒ ู ฺฉุด

### AI Response Cache

```python
# unified_ai/services/cache_service.py

class AIResponseCache:
    """ฺฉุด ูพุงุณุฎโูุง AI ุจุฑุง ุจูููโุณุงุฒ"""
    
    def __init__(self):
        self.redis_client = get_redis_client()
        self.ttl = {
            'chat': 3600,      # 1 hour
            'medical_info': 86400,  # 1 day
            'translation': 604800   # 1 week
        }
        
    async def get_cached_response(
        self,
        query: str,
        service: str,
        context: Optional[Dict] = None
    ) -> Optional[str]:
        """ุจุงุฒุงุจ ูพุงุณุฎ ุงุฒ ฺฉุด"""
        
        # ุชููุฏ ฺฉูุฏ ฺฉุชุง
        cache_key = self._generate_cache_key(query, service, context)
        
        # ุจุงุฒุงุจ ุงุฒ ฺฉุด
        cached = await self.redis_client.get(cache_key)
        
        if cached:
            # ุจูโุฑูุฒุฑุณุงู ุขูุงุฑ
            await self._increment_cache_hit(service)
            return json.loads(cached)
            
        return None
        
    async def cache_response(
        self,
        query: str,
        response: str,
        service: str,
        context: Optional[Dict] = None
    ):
        """ุฐุฎุฑู ูพุงุณุฎ ุฏุฑ ฺฉุด"""
        
        # ุจุฑุฑุณ ูุงุจูุช ฺฉุด
        if not self._is_cacheable(query, service):
            return
            
        cache_key = self._generate_cache_key(query, service, context)
        ttl = self.ttl.get(service, 3600)
        
        await self.redis_client.setex(
            cache_key,
            ttl,
            json.dumps({
                'response': response,
                'cached_at': datetime.utcnow().isoformat(),
                'service': service
            })
        )
        
    def _generate_cache_key(
        self,
        query: str,
        service: str,
        context: Optional[Dict]
    ) -> str:
        """ุชููุฏ ฺฉูุฏ ฺฉุด ฺฉุชุง"""
        
        # ูุฑูุงูโุณุงุฒ query
        normalized_query = query.lower().strip()
        
        # ูุด ฺฉุฑุฏู
        content = f"{service}:{normalized_query}"
        if context:
            content += f":{json.dumps(context, sort_keys=True)}"
            
        return f"ai_cache:{hashlib.md5(content.encode()).hexdigest()}"
        
    def _is_cacheable(self, query: str, service: str) -> bool:
        """ุจุฑุฑุณ ูุงุจูุช ฺฉุด ุดุฏู"""
        
        # ุณูุงูุงุช ุดุฎุต ฺฉุด ููโุดููุฏ
        personal_indicators = [
            'ูู', 'ูุชุงุฌ ุขุฒูุงุด ูู', 'ูพุฑููุฏู ูู',
            'ุณุงุจูู ูู', 'ุฏุงุฑููุง ูู'
        ]
        
        return not any(indicator in query for indicator in personal_indicators)
```

---

[ELEMENT: div align="center"]

[โ ูุจู: ุงุญุฑุงุฒ ููุช ฺฉูพุงุฑฺู](05-authentication.md) | [ุจุนุฏ: ุณุณุชู ูุงู ู ุงุดุชุฑุงฺฉ โ](07-billing-system.md)

</div>
